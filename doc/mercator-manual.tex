\documentclass[11pt]{article}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{syntax}
\usepackage{multicol}
\usepackage{hyperref}

\newcommand{\sizet}{size\textunderscore{}t}

\title{MERCATOR Reference Manual, v0.9}

\begin{document}

\maketitle

Copyright (C) 2019 Washington University in St. Louis.  All rights
reserved.

MERCATOR is licensed under the Apache License, Version 2.0 (the
"License"); you may not use this software except in compliance with
the License.  You may obtain a copy of the License at

  \url{http://www.apache.org/licenses/LICENSE-2.0}
	
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


Development of MERCATOR has been sponsored by NSF CISE awards
CNS-1500173 and CNS-1763503.

When using MERCATOR, please cite the following publication:

S. V. Cole and J. Buhler, "MERCATOR: A GPGPU Framework for Irregular
Streaming Applications," \textit{2017 International Conference on High
  Performance Computing \& Simulation (HPCS)}, Genoa, 2017,
pp. 727-736.

\newpage

\section{Overview and Key Concepts}

\section{Building MERCATOR}

Build requirements for MERCATOR include:
\begin{itemize}

\item CUDA 10.2 or higher

\item The CUB (CUDA Unbound) library, 1.8.0 or higher

\item LLVM/Clang libraries v9 or higher (for type-checking)

\item GNU Flex 2.5.37 or higher (older versions might work)

\item GNU Bison 3.0.4 or higher (older versions might work)

\item CMake 3.8 or higher

\end{itemize}
Older versions of some of these tools may still work, but the CUDA
installation and C++ compiler must fully support C++14.  MERCATOR has
been tested with GCC 8.3 as the host compiler.  GCC earlier than v6
and LLVM prior to v8 may cause problems.

MERCATOR applications will run on devices with CUDA architecture 3.0
(Kepler) or higher.

Assume the MERCATOR sources have been unpacked into directory \texttt{source}.
To build and install MERCATOR, do the following:
\begin{enumerate}

\item If needed, edit the user-configurable paths near the top of the file
      \texttt{source/CMakeLists.txt}.  In particular, edit the following:
 \begin{itemize}
   \item \texttt{CMAKE_INSTALL_PREFIX} -- the place where MERCATOR will be
          installed
   \item \texttt{LLVM_PATH} -- path to the local LLVM installation
 \end{itemize}

\item Create and enter a build directory separate from the source tree, e.g.
\begin{quote}
\texttt{mkdir build ; cd build}
\end{quote}

\item Initialize the CMake build files:
\begin{quote}
\texttt{cmake /path/to/source}
\end{quote}

\item Build MERCATOR:
\begin{quote}
\texttt{make}
\end{quote}

\item Install MERCATOR:
\begin{quote}
\texttt{make install}
\end{quote}

\end{enumerate}

You may wish to build the MERCATOR examples in the source tree as
well.  The procedure to do this is similar to that used to build
MERCATOR itself.  You can build using an installed version of
MERCATOR, or a version that was compiled but not yet installed.
\begin{enumerate}

\item If needed, edit the user-configurable paths near the top of the
  file \texttt{source/examples/CMakeLists.txt}.  In particular, edit
  the following:

 \begin{itemize}
   \item \texttt{MERCATOR_ROOT_DIR} -- path to the MERCATOR installation,
      if installed, or to the source tree, if not

   \item \texttt{BUILDING_IN_TREE} -- 
      set true in order to build using a compiled but not-yet installed
      version of MERCATOR

   \item \texttt{MERCATOR_BUILD_DIR} --
      if \texttt{BUILDING_IN_TREE} is true, path to the build directory
      for the noet-yet-installed MERCATOR

   \item \texttt{CUB_ROOT_DIR} -- path to the local installation of CUB
 \end{itemize}

\item Create and enter a build directory separate from the source tree, e.g.
\begin{quote}
\texttt{mkdir build ; cd build}
\end{quote}

\item Initialize the CMake build files:
\begin{quote}
\texttt{cmake /path/to/source/examples}
\end{quote}

\item Build the examples:
\begin{quote}
\texttt{make}
\end{quote}

\end{enumerate}

\section{Compiling a MERCATOR App}

The steps needed to build a MERCATOR app are as follows.
\begin{enumerate}
\item Write a \emph{specification file} describing the app's topology.

\item Compile the app with the MERCATOR compiler using the \texttt{-K}
  option to produce a \emph{skeleton file} with methods to be filled
  in by the user.

\item Fill in the skeleton's methods with valid CUDA code.

\item Write a host-side \emph{driver} (in C++ or CUDA) that
  instantiates the app and runs it.  The driver may be part of
  a larger program and may instantiate and run multiple MERCATOR
  apps.

\item Build the driver code together with the filled-in skeleton
  code and runtime support files generated by the MERCATOR compiler.

\end{enumerate}
The format of a MERCATOR specification file is described in the next
section.  Here, we describe the behavior of the MERCATOR
compiler and the set of files it produces.

\subsection{MERCATOR Compiler Inputs and Outputs}

The MERCATOR compiler reads one or more specification files and
produces outputs for each app specified in each file.  If the file
specifies an app named, say, \texttt{App}, then:
\begin{itemize}

\item If the \texttt{-K} flag is passed to the compiler, it emits a
  skeleton file \texttt{App.cu.skl}, which contains stubs of the app's
  device-side methods to be filled in by the user.

\item If the \texttt{-K} flag is not passed, then the compiler emits

\begin{itemize}

\item a host-side header \texttt{App.cuh}, which should be included
  by any host-side code that instantiates or references the app
  (in particular, the host-side driver);

\item an initialization source file \texttt{App_init.cu}, which should
  be compiled together with the driver.  All device-side code
  generated by the MERCATOR compiler and runtime will be instantiated
  in this file.

\item A device-side header \texttt{App_dev.cuh}, which is already
  included by the other generated files and need not be included
  anywhere else.

\end{itemize}

\end{itemize}

The user should fill in the methods in the skeleton file and rename it
to a CUDA file, say \texttt{App.cu}, which should be compiled together
with the initialization source file and the user's driver code.  Note
that \emph{compiling a spec file overwrites any previous outputs from
  that file with no warning}, so a skeleton file should not be left
with its original name after being edited.

An app's device-side code is split between its (filled-in) skeleton
file and the initialization source file.  These two files can either
be compiled separately by nvcc with the \texttt{-dc} option or
included by reference (along with any other device code used by the
app) in a single compilation unit to avoid the runtime overhead
incurred by separate compilation.

Compilation of both the device code and code that includes the
host-side header must be able to find the MERCATOR runtime.  Be sure
to add the runtime directory of your MERCATOR installation to the
compiler's include path.  No special libraries are needed beyond those
required to build CUDA applications.

\subsection{MERCATOR Compiler Command-Line Syntax}

The following command-line syntax is recognized by the MERCATOR
compiler.  Pass the \texttt{-h} flag to the compiler to see a help
message.
\begin{quote}
\texttt{mercator [ \textit{options} ] \textit{specfile} [ \textit{specfile} ... ]}
\end{quote}
 
The following options are recognized by the MERCATOR compiler:
\begin{itemize}

\item \texttt{-a \textit{app}} \\
   given spec file(s) defining multiple applications, generate output
   for only the single specified app.

\item \texttt{-I \textit{path}} \\
   add the specified path to the include path for finding files
   mentioned by reference statements in a spec file.  By default,
   MERCATOR checks the system include path of the local LLVM
   installation, the CUDA incude path, and the directory where the
   spec file is located.

\item \texttt{-K} \\
   for each app, emit a skeleton file for the user to fill in
    (default: emit runtime support code for each app)

\item \texttt{-o \textit{path}} \\
   write output files to the specified path (defaults to current directory).

\item \texttt{-t \textit{\#}} \\
  set the number of threads per block for each generated application
  (default 128).  This number must be fixed at the time the app is
  compiled.
  
\item \texttt{-H \textit{\#}} \\ 
  set the size of the device heap in megabytes (default 32 MB).
  Values less than the default are not recommended and may result in
  app launch failures.  If several instances of the app may be
  constructed at once, the heap size should be increased by a factor
  of the number of instances.  Additional space may be needed if the
  user's own code makes use of the device heap.

\item \texttt{-S \textit{\#}} \\
  set the size of the device stack in kilobytes  (default 8 KB).
  Values less than the default are not recommended and may result
  in app launch failures.

\item \texttt{-D} \\
  do not generate code for any apps, but emit dependencies for
  each app to be generated.  Dependencies have the form
\begin{quote}
  \textit{sourcefiles} : \textit{specfile}
\end{quote}
where ``sourcefiles'' includes the host and device headers and the
initialization source file (but not the skeleton).  One such line is
generated per app specified in the input.

\item \texttt{-v} \\
   print MERCATOR version information.

\end{itemize}

\subsection{CMake Support for Building MERCATOR Applications}

MERCATOR provides some CMake recipes to simplify building and using
applications.  These recipes can be included in your CMake build
system by including the file \texttt{mercator-rules.txt} in the
top-level MERCATOR install directory.

To use the MERCATOR recipes, you must define two variables in your
\texttt{CMakeLists.txt} file:
\begin{itemize}
 \item \texttt{MERCATOR_ROOT_DIR} -- path to the MERCATOR installation

  \item \texttt{CUB_ROOT_DIR} -- path to the local installation of CUB
\end{itemize}
In addition, MERCATOR applications are built using CMake 3.8+'s CUDA
support, so you may wish to set the various CUDA configuration options.

the \texttt{mercator-rules.txt} file defines two CMake functions:
\begin{itemize}

\item
\begin{verbatim}
  add_mercator_app(TARGET <appname>
                   SPECFILE <filename>
                   SOURCES <filename> [<filename> ...])
\end{verbatim}

Build a MERCATOR application from a specification file and one or more
user-provided sources (which should include the filled-in skeleton).
The application name should be the target.  CMake will construct a
static library \texttt{lib<appname>.a} containing all the device-side
code for the application and will generate all the necessary support
code and headers as described above.

\item
\begin{verbatim}
  add_mercator_executable(TARGET <exename>
                          APPS <app> [<app> ...]
                          SOURCES <filename> [<filename> ...])
\end{verbatim}

Build an executable that uses one or more MERCATOR applications.
The app names should be targets created using \texttt{add_mercator_app}.
The other sources may be arbitrary C++ or CUDA files.  CMake will
ensure that the apps' headers and libraries are built before building
the executable, and that it is linked against the apps' libraries.

\end{itemize}

\section{MERCATOR App Specification File Format}

This section describes the format of MERCATOR app specifications.

\subsection{Lexical Elements}

A MERCATOR app specification consists of lexical tokens optionally
separated by white space. The amount and location of white space between
tokens, including newlines, does not matter.  Specifications may use
C++ style comments (e.g.\ ``\texttt{// comment}''); all text on a line
after the two-character sequence ``\texttt{//}'' is ignored.

\textit{Numbers} in a MERCATOR spec are unsigned decimal integers
consisting only of the digits 0 to 9.

MERCATOR specifications name a variety of entities using
\textit{identifiers}.  A valid identifier is a string containing only
alphanumeric characters or the underscore symbol ``\texttt{_}'' and
not beginning with a number.  Identifiers are case-sensitive.
Identifiers beginning with two underscores ``\texttt{__}'' are
reserved for use by the system.

MERCATOR reserves the following keywords, which are
case-\emph{in}sensitive.  These keywords may not be used as
identifiers.
\begin{multicols}{4}
\begin{itemize}
 \item \texttt{aggregate}
 \item \texttt{allthreads}
 \item \texttt{application}
 \item \texttt{edge}
 \item \texttt{enumerate}
 \item \texttt{from}
 \item \texttt{ilimit}
 \item \texttt{mapping}
 \item \texttt{module}
 \item \texttt{node}
 \item \texttt{nodeparam}
 \item \texttt{nodestate}
 \item \texttt{param}
 \item \texttt{reference}
 \item \texttt{source}
 \item \texttt{sink}
 \item \texttt{void}
\end{itemize}
\end{multicols}

MERCATOR specifications may sometimes need to refer to C++ types,
which are specified as \textit{typenames}. Anywhere a typename is
needed, it may be given as a \emph{type literal} enclosed in curly
braces ``\texttt{\{\}}'', within which any character other than a
newline is permitted and considered part of the type.  For example,
the following are valid type literals:
\begin{itemize}
\item ``\texttt{\{int\}}''
\item ``\texttt{\{const unsigned int * const []\}}''
\item ``\texttt{\{Foo<Bar, Baz<Quux>, 3> *\}}''
\end{itemize}
Simple types whose names are either valid MERCATOR identifiers or
pointers to them may be specified without braces.  For example, the
following types may be specified unbraced:
\begin{itemize}
\item \texttt{int}
\item \texttt{My_Type_Name}
\item \texttt{float *}
\item \texttt{My_Type_Name *}
\end{itemize}

\subsection{Grammar}

A MERCATOR specification file describes zero or more applications.
The file is divided into sections, each beginning with an
\emph{application statement} of the form
\begin{quote}
  \texttt{application <identifier> ;}
\end{quote}
All statements after an application statement and before the next such
statement refer only to the named application, except that any
reference statements (see below) apply to every application specified
in the file.

An application specification declares one or more \emph{nodes}, each
of which has a \emph{module type} that specifies its input and output
properties.  Nodes are connected into an application topology by
\emph{edges}.  Node, module, and edge statements, as well as the
various other specialized statement types described below, may appear
in any order within an application section.

\subsubsection{Reference Statement}

All typenames mentioned in a specification file must be well-defined.
Typenames are interpreted by the LLVM Clang C++ compiler.  To include
an external C++ or CUDA file that defines typenames used in the
specification, use a \textit{reference statement}:
\begin{quote}
\texttt{reference "<filename>" ;}
\end{quote}
\texttt{<filename>} is the name of a file that will be read by Clang before
attempting to resolve any typenames in the specification.  CUDA files
are parsed in host mode, not device mode.  Any number of reference
statements may be used; the contents of all referenced files will be
parsed in a single translation unit and used for type checking.

Referenced files may be specified by absolute or relative pathnames.
Relative pathnames are assumed to be relative to either the current
working directory or a directory in the system include path (as
defined by the LLVM installation).  The include path can be augmented
by passing additional directories to the MERCATOR compiler with
the \texttt{-I} option.

\subsubsection{Module Statement}

A \textit{module statement} names a module and specifies its type.
The following grammar describes the syntax for module statements:
\begin{grammar}

<module-stmt> ::= `module' <module-name> `:' <module-type> `;'

<module-name> ::= <identifier>

<module-type> ::= <input-type> `->' <output-types>

<input-type> ::= <typename>

<output-type> ::= void
\alt <channel-spec>
\alt <channel> [ `,' <channel> ... ]

<channel> ::= <channel-name> `<' <channel-spec> `>'

<channel-spec> ::= <typename> [ `:' <output-count> ]

<channel-name> ::= <identifier>

<output-count> ::= <number> [ `!' ]

\end{grammar}

A module has an input data type and zero or more output
\emph{channels} on which the module can emit data.  A channel has a
name (optional if it is a module's only channel), an output data type,
and an optional \emph{output count} specifying the maximum number of
output items that can be generated on this channel for each input item
consumed by the module.  The output count is a maximum value; the
module may produce fewer outputs (but may not produce more) than the
specified count for each input.  If no output count is specified, it
is assumed to be 1, i.e.\ the module produces at most one output per
input.  Adding the symbol ``\texttt{!}''  after an output count
indicates that the module produces \emph{exactly} this many outputs
for each input.

Output channel names must be unique within one module type but may be
duplicated across module types.

% NB: the lottery scheduler, which we don't use right now, is limited
% to apps with <= THREADS_PER_BLOCK module types.  The maxOcc
% scheduler does not have that limitation.  If we ever want to use the
% lottery scheduler, we can do a bit of work to remove the limit.

\subsubsection{Node Statement}

A \textit{node statement} specifies a node of the MERCATOR application
graph, along with its type.  The following grammar describes the
syntax for node statements:
\begin{grammar}

<node-stmt> ::= `node' <node-name> `:' <node-type> `;'

<node-name> ::= <identifier>

<node-type> ::= <identifier>
\alt `source' `<' <typename> `>'
\alt `sink'   `<' <typename> `>'

\end{grammar}

A node's type is either the name of a module (specified in a
module statement) or a \emph{source type} or \emph{sink type}, which
respectively receive an external data stream or emit a data stream
from the application.  Source and sink node types are parameterized by
their data types, which can be any valid typename.

A node whose type is a module assumes the input data type and output
channel list of the module.  A node whose type is a source type has no
inputs (since it receives input from an external source) and has
single output channel named \texttt{out} with the specified data type.
A node whose type is a sink type has the specified input data type and
no output channels (since data sent to it leaves the application).

Note that node and module names are in the same namespace. Hence, the
same name may not be used for both a module and a node within one
application.

\textit{Limitation}: the number of distinct nodes of a given module
type may not exceed 32 (the GPU warp size).

% This is a difficult limit to remove.  If needed in the future, we
% can silently split the instances of a module into groups and
% instantiate them as separately scheduled module objects.

\subsubsection{Edge Statement}

An \emph{edge statement} specifies a connection between two nodes.
The following grammar describes the syntax for edge statements:
\begin{grammar}

<edge-stmt> ::= `edge' <node-name> [`::' <channel-name>] `->' <node-name> `;'

<node-name> ::= <identifier>

<channel-name> ::= <identifier>

\end{grammar}

An edge statement connects a specified output channel of an
\emph{upstream} node to the input of a \emph{downstream} node. If
the upstream node has only a single output channel, the channel
name is optional; the node's unique output channel is assumed.

The two endpoints of an edge must have compatible types; more
specifically they must be equivalent C++ types up to aliasing (no
automatic conversions are applied).  It is an error to specify an edge
with a sink node as the upstream endpoint or the source node as a
downstream endpoint.

\subsection{Topological Properties of Applications}

Not every possible graph structure that can be specified using node
and edge statements is a valid MERCATOR application. Valid application
graphs have the following properties:

\begin{enumerate}

\item An application has exactly one source node.

\item Every node must be reachable from the source.

\item No node may have two distinct incoming edges, unless one of them
  is part of a directed cycle involving that node.

\item No node may be part of more than one directed cycle.

\item Every output channel that is part of a directed cycle must
  produce no more than one output per input received by its node.

\end{enumerate}

The last three properties permit simple directed cycles in an
application graph but forbid more complex cycle structures and
arbitrary directed acyclic graphs.  Effectively, an application must
be a connected tree rooted at its source node, except that back edges
from a node to one of its ancestors in the tree are permitted.
However, if there is a back edge from a node $v$ to its ancestor $u$,
no other back edge may originate from or terminate at any node on the
path from $u$ to $v$ (inclusive).

It is permissible to define a module type with no nodes of that type,
or to leave an output channel of a node unconnected to an edge. In the
former case, the module type is not emitted as part of the generated
application; in the latter, outputs sent to the channel will be
silently discarded. The MERCATOR compiler will generate warnings in
such cases.

\paragraph*{Rationale for Topological Restrictions}
Edges in MERCATOR application graphs are implemented by finite,
fixed-sized queues.  Without careful reasoning about the amount of
space available in each edge's queue, the runtime system might
schedule the execution of nodes in a way that leads to \emph{deadlock}
-- a condition in which at least one node has queued inputs but no
node has room for its outputs on its downstream edges.  The
topological restrictions on application graphs ensure that relatively
simple, efficient scheduling rules suffice to prevent deadlock for all
valid topologies.  Future versions of MERCATOR might relax these
restrictions if workable scheduling rules are found that prevent
deadlock for more general graph topologies.

The restriction that each channel in a cycle must produce at most one
output per input ensures that one item entering a cycle at a node $v$
cannot result in two or more items appearing on the back edge into
$v$.  If such amplification of inputs were permitted, it could again
lead to deadlock with fixed-sized queues.

\subsubsection{Additional Properties of Modules}

MERCATOR module types have certain additional properties that can be
specified in optional statements separate from the module type
declaration.

\paragraph*{Input Limit Statement}
An \emph{input limit statement} limits the number of threads that
may concurrently receive inputs in a MERCATOR module's \texttt{run()}
function.  This statement has the form
\begin{quote}
\texttt{ilimit <module-name> <number> ;}
\end{quote}
which states that a call to \texttt{run()} for the named module will
execute with inputs in at most the specified number of threads.  By
default, the number of threads with inputs can be as large as the
GPU block size.

Input limits are useful when each input concurrently processed by
a module consumes scarce resources such as GPU shared memory.  It
may be necessary to limit the number of inputs processed to avoid
exhausting such resources.

\paragraph*{Mapping Statement}
A \emph{mapping statement} changes the mapping of input items to
threads for a particular module's \texttt{run()} function.  It is
possible to specify that multiple GPU threads should cooperatively
process an input item, or that multiple input items should be
processed sequentially by each thread.

the mapping statement has the form

\begin{quote}
\texttt{mapping <module-name> <nelts> :~<nthreads> ;}
\end{quote}

The parameter \textit{nelts} specifies the maximum number of input items
delivered to each group of GPU threads, while \textit{nthreads} specifies the
size of a group.  For example, a mapping specification
``\texttt{2:4}'' would deliver up to two inputs to each group of four
consecutive GPU threads.  All threads in each group receive the same
two inputs; the first thread in the group is responsible for pushing
any outputs from the group to downstream channels.

\emph{At this time, specifying more than one input item per thread is not
supported.}  A future version of MERCATOR may lift this restriction.
However, thread group sizes $> 1$ are permitted.  If the number of threads
in a block is not a multiple of the group size, the residual threads that
are not part of a full group will not receive inputs.

By default, each active thread in a call to \texttt{run()} receives
one input; that is, the default mapping is ``\texttt{1:1}''.

\paragraph*{AllThreads Statement}
An \emph{allthreads statement} affects the behavior of a module's
\texttt{run()} function.  The form of this statement is
\begin{quote}
\texttt{allthreads <module-name> ;}
\end{quote}

By default, if a call to a module's \texttt{run()} has enough inputs
to supply $n$ GPU threads with work, the function will be called with
only these $n$ threads active.  $n$ may be less than the GPU block
size and will vary at runtime.  This arrangement is simple and
convenient if each thread processes its input(s) independently of all
other threads. 

However, more advanced GPU usage may require that all threads in a GPU
block be active in each call to \texttt{run()}.  In particular,
cooperative behaviors spanning multiple GPU threads, which require a
call to \texttt{__syncthreads()}, are not safe unless every thread in
the block eventually reaches the synchronization point.  In such
cases, an allthreads statement instructs MERCATOR to call
\texttt{run()} with all threads active, regardless of the number of
inputs to be processed.  The user must then manually check whether
each thread has received inputs by inspecting the thread's
\texttt{nodeIdx} argument.

\textit{NB}: the allthreads statement is a temporary accommodation
that will become obsolete once MERCATOR is adapted to use CUDA 9's
cooperative thread groups facility, which permits safe synchronization
of sub-block-sized sets of threads.

\subsubsection{Application, Module, and Node Parameters}

MERCATOR applications can be configured at runtime by setting
\emph{parameters} whose values are accessible by modules running on
the GPU device.  Parameters can be read and written from the host
before each run of a MERCATOR application but are unchanging and
read-only from the GPU during application's execution.

There are three kinds of parameters that can be defined for a
MERCATOR application.   \emph{App-wide} parameters are shared by
all modules in an application.  \emph{Module-wide} parameters are
shared by all nodes of a given module type.  Finally, \emph{Node}
parameters are defined for all nodes of a module type but may have
different values for each node.

App-wide and module-wide parameters are defined using one or more
\textit{parameter statements} of the following form:
\begin{grammar}

<param-stmt> ::= `param' <param-name> `:' <param-type> `;'

<param-name> ::= [ <module-name> `::' ] <identifier>

<param-type> ::= <typename>

\end{grammar}

A parameter statement names a parameter and specifies its data type.
The parameter name may be a bare identifier, in which case it is
app-wide, or it may be scoped to a particular module, in which case
it is module-wide.

Node parameters are defined using the analogous \textit{node parameter
statement} of the following form:
\begin{grammar}

<nodeparam-stmt> ::= `nodeparam' <param-name> `:' <param-type> `;'

<param-name> ::= <module-name> `::' <identifier>

<param-type> ::= <typename>

\end{grammar}

Other than using a different keyword, the main difference between
parameter and node parameter statements is that the latter must be
scoped to a particular module.  Each node of the module type will have
a separate copy of the named parameter whose value can be set
independently from the host.

\subsubsection{Node State}

Unlike parameters, which are initialized on the host CPU and are
read-only on the GPU, \emph{state} variables are initialized on the
GPU and are read-write.  They are not accessible from the host at all.
State variables are always defined per-node.

Node state is defined using a \textit{node state statement} as follows:
\begin{grammar}

<nodestate-stmt> ::= `nodestate' <var-name> `:' <var-type> `;'

<var-name> ::= <module-name> `::' <identifier>

<var-type> ::= <typename>

\end{grammar}

\paragraph*{Rationale for State}
State variables make sense only if they can be updated in a
sequentially consistent fashion. If a state variable were shared
across multiple nodes, there would be no way to guarantee the order
in which different nodes update the variable, since the order in
which modules are executed varies dynamically at runtime.  Hence,
MERCATOR does not define per-module or per-application state.

In contrast, for acyclic applications running within a single GPU
block, there is a consistent notion of ``stream order.''  Edges do not
reorder their inputs, and items earlier in a node's input queue are
always assigned to lower-numbered threads within a single call to
\texttt{run()}.  Hence, a single node always processes its inputs in
stream order, and it can therefore maintain a state variable's value
to reflect the state after processing some prefix of the stream.
In practice, a MERCATOR application divides its input stream across
GPU blocks in an unpredictable fashion, so stream order is meaningful
only with respect to a single aggregate (to be implemented soon),
whose elements are guaranteed to be processed within a single block.

Moreover, for applications with cycles, MERCATOR does not guarantee
that items that enter a cycle first will exit the cycle first, since
any one item may go around the cycle a variable number of times.
Waiting for earlier items to exit the cycle before accepting later
items could have a serious negative impact on thread occupancy, since
one long-lived item would prevent any later items from being
processed.  A somewhat performance-preserving solution would be to
reorder items into stream order as they exit the cycle, using a
reorder buffer.

\section{Host-Side App Interface}

This section describes how C++ and CUDA host-side code can instantiate
and use MERCATOR applications.  

\subsection{A Usage Example}

The following example shows a C++ source file that instantiates and runs
a MERCATOR application named \texttt{MyApp}.  Suppose that \texttt{MyApp}
was compiled from the following specification:
\begin{verbatim}
application MyApp;

ModuleType Foo : int -> out<int> ;

Node src : Source<int> ;
Node foo : Foo ;
Node snk : Sink<int> ;

Edge src -> foo;
Edge foo -> snk;

Param y : float;
Param Foo::x : int;

\end{verbatim}
The following example, which will be explained fully below, shows how
to create and use an instance of the \texttt{MyApp} application:
\begin{verbatim}
#include "MyApp.cuh"

int myFunction()
{
  // host-side buffers
  int *input = new int [100];  
  int *output = new int [100];  

  Mercator::Buffer<int> ib(100);
  Mercator::Buffer<int> ob(100);

  MyApp myApp;

  myApp.getParams()->y = 2.0;
  myApp.Foo.getParams()->x = 3;

  myApp.src.setSource(ib);
  myApp.snk.setSink(ob);

  // generate some input data
  for (unsigned int j = 0; j < 100; j++)
    input[j] = j;

  ib.set(input, 100);
  myApp.run();
  ob.get(output, ob.size());

  // print any results
  for (unsigned int j = 0; j < ob.size(); j++)
    cout << output[j];
}
\end{verbatim}

\subsection{Version Information}

The following values are defined in any source file that includes an
app's host-side header.  They are also accessible in the device-side
stub code.

\begin{itemize}

\item \texttt{MERCATOR_MAJOR} -- major version of MERCATOR

\item \texttt{MERCATOR_MINOR} -- minor version of MERCATOR

\item \texttt{MERCATOR_PATCHLEVEL} -- patch level of MERCATOR

\end{itemize}

API changes will result in an increment to the major and/or minor
versions.  Patch level changes are intended to be source-compatible.


\subsection{Instantiating a MERCATOR app}

To instantiate a MERCATOR app, the user should do two things: include
the generated header file that defines the host-side app interface,
and declare an object of the app's type.  The host-side app interface
is defined in the CUDA header file with the same name as the app
itself (in this case, \texttt{MyApp.cuh}).  The corresponding
device-side app header (whose name ends in \texttt{_dev.cuh}) should
\emph{not} be included in host-side code.

% Note: the host-side header could be a plain old .h file, except that
% it currently includes the Source and Sink headers, which hold
% template classes containing CUDA code.  Defining these classes without
% exposing the CUDAism to the user seems challenging -- we'd need to
% have a CUDA-free pure virtual interface (still parameterized by T)
% and an implementation class, not exposed to the user, that has
% all the stuff in the existing classes (and generates the key pointer
% passed off to the device, which would need to be opaque to the user).

A MERCATOR application is instantiated by creating an object of its
type.  The app object may simply be declared, as in the example, and
need not be allocated with \texttt{new}.  The app constructor takes
two optional arguments:
\begin{quote}
\texttt{App(cudaStream_t stream = 0, int deviceId = -1)}
\end{quote}
the first argument is the CUDA stream in which to run the app, and
the second is the CUDA device to which to bind it.  If the stream is
not specified, the default stream (0) is used.  If the device is not
specified, the current CUDA device will be used.

The stream and device settings for an app are used any time it is run.
The device setting is immutable once the app is constructed, because
the app maintains state on the device even between runs.  The stream
of an app may be changed at any time by calling the method
\begin{quote}
\texttt{bindToStream(cudaStream_t stream)}
\end{quote}
This call will block until all pending operations on the app's
previous stream are complete, then switch to thee new stream.

\subsection{Querying App Properties}

Presently, the only property of an app that can be queried from the
host is the number of GPU blocks that it will use to execute.  This
property can be accessed by the following function: two optional
arguments:
\begin{quote}
\texttt{int getNBlocks() const}
\end{quote}
This property is set when an app is instantiated and remains
constant for a given instance.

% FOR LATER -- query bound device and maybe stream?
%              also perhaps heap and stack size?
%              we may want to rethink the interface if we are
%              going to add a lot of accessors, since each
%              one currently needs its own function in the codegen'd class

\subsection{Setting the App's Parameters}

Before running a MERCATOR app, the user must set its app-level,
module-level, and node-level parameters.  The application object
provides type-safe interfaces for parameter setting.

For each module and each node declared in the application, an object
with the same name as the module/node is defined as a public member of
the application's class.  Other than source and sink nodes, each
object provides a method \texttt{getParams()} that returns a pointer
to a writable \emph{parameter structure}, which contains an
appropriately typed member for each parameter associated with the
module/node.  Module-level parameters are accessed through the object
with the same name as the module, while node-level parameters are
accessed through the object with the same name as the node.  App-level
parameters are accessed using the \texttt{getParams()} method of the
entire app object.

\textit{Note}: the type of the parameter structure returned from
\texttt{getParams()} is different for every app, module, and node
object.  If the user wishes to store this pointer locally (e.g.\ to
set several parameters of a module all at once), a variable of type
\texttt{auto} should be be used to hold the pointer.

Source and sink nodes have a special interface, described below,
through which the user can supply input to and receive output from the
application.

\subsection{Input and Output}

Data is exchanged between the host and a MERCATOR application using
typed objects, which may be heap- or stack-allocated.  There are two
kinds of objects used for data exchange: \textit{buffers} and
\textit{ranges}.

\subsubsection{Buffers}

A MERCATOR buffer, which has type \texttt{Buffer<T>}, is a handle for a
piece of device-side memory that can hold up to a specified number of
items of type \texttt{T}.  Buffer objects can be read and written
by user code explicitly and can be used as input or output of
an application.

A buffer's constructor takes a single argument, which is the maximum
number of items that the buffer will hold (its \textit{capacity}).  A
buffer can also be built via copy constructor or \texttt{operator=}
from an existing buffer, which makes a copy of this buffer.

Buffers support the following basic methods:
\begin{itemize}
\item \texttt{\sizet{} capacity() const} \\
  Return the buffer's capacity.
  
\item \texttt{\sizet{} size() const} \\
  Return the number of items actually present in the buffer.

\item \texttt{void get(T *p, \sizet{} n, \sizet{} offset = 0) const} \\
  Copy $n$ items from the buffer to host memory $p$, starting at
  a specified offset in the buffer.

\item \texttt{void set(const T *p, \sizet{} n)} \\
  Copy $n$ items pointed to by $p$ to the buffer, and set its size to $n$.
  The old size and content of the buffer is lost. 

\item \texttt{void copy(const Buffer<T> *other, \sizet{} n)} \\
  Copy $n$ items from the buffer ``other'' to the current buffer, and set
  its size to $n$. The old size and content of the buffer is lost. 
  This call is done without touching the host's memory.

\item \texttt{void clear()} \\
  Reset the buffer's size to 0.

\end{itemize}
The lasxt four operations are \emph{synchronous} -- they do not return
control to the calling host thread until all prior operations on the
device are complete. Buffers also support asynchronous variants of these
operations, all of which take a CUDA stream as an optional argument:
\begin{itemize}

\item \texttt{void getAsync(T *p, \sizet{} n, \sizet{} offset = 0, cudaStream_t stream = 0) const}

\item \texttt{void setAsync(const T *p, \sizet{} n, cudaStream_t stream = 0)}

\item \texttt{void copyAsync(const Buffer<T> *other, \sizet{} n, cudaStream_t stream = 0)}

\item \texttt{void clearAsync(cudaStream_t stream = 0)}

\end{itemize}
All these functions are executed asynchronously with respect to the
host.  They do not synchronize the device, but each operations will
execute \emph{only} after all previous operations in the same stream
have completed.  Additionally, if the host pointer for
\texttt{getAsync} or \texttt{setAsync} points to page-locked memory,
then these operations may overlap with operations in another stream.

\textit{Warning}: buffers are always allocated on the current GPU device.
It is an error to use a buffer allocated on one device as input or output
to an app bound to a different device.  In such cases, use the \texttt{copy}
function to copy the buffer's data to the correct device first.

\subsubsection{Ranges}

A MERCATOR range, which has type \texttt{Range<S>}, describes a range
of equally-spaced numerical values using only constant space.
\texttt{S} must be an \textit{arithmetic type}, that is, a numerical
type supporting addition, subtraction, multiplication, and division.
It is an error to instantiate a range with a non-arithmetic type.

A range's constructor has the following interface:
\begin{quote}
\texttt{Range<S>(S start, S end, S step, bool includeEndpoint = false)}
\end{quote}
which includes every point in the specified interval that can be 
obtained by adding \textit{step} to start 0 or more times.
If \texttt{includeEndpoint} is false, the interval in question is
the half-open interval $[\textit{start}, \textit{end})$; if true,
it is the closed interval $[\textit{start}, \textit{end}]$.
For instance,
\begin{itemize}
\item \texttt{Range<int>(1, 100, 2)} specifies all the odd non-negative
      integers less than 100.

\item \texttt{Range<int>(1, 101, 2)} specifies the same range as
      the previous statement.

\item \texttt{Range<int>(1, 101, 2, true)} specifies the range of
      the previous statement \emph{plus} the point ``101''.

\item \texttt{Range<float>(0.0, 1.0, 0.01)} specifies all the values
      in $[0.0, 1.0)$ that are multiples of 0.01.
\end{itemize}

The number of elements in a range may be obtained by calling the
method
\begin{quote}
\texttt{size_t Range<S>() const}
\end{quote}
 
\subsubsection{Associating Objects With Source and Sinks}

The source node of a MERCATOR application can be associated with a
buffer or range object.  To associate one of these objects with a
source node, pass the object to the node's \texttt{setSource} method:
\begin{itemize}

\item \texttt{void setSource(const Buffer<T> \&buffer)} \\
  Associate a buffer with the source node.

\item \texttt{void setSource(const Range<T> \&range)} \\
  Associate a range with the source node.

\end{itemize}

A sink node can only be associated with a buffer, using the 
\texttt{setSink} method:
\begin{itemize}

\item \texttt{void setSink(Buffer<T> \&buffer)} \\
  Associate a buffer with the sink node.

\end{itemize}
Any associations set for an application's source and sinks, like
changes to its parameters, take effect next time the application is
run.

\subsection{Running an Application}

Once all parameters of a MERCATOR application, including its source
and sink data, have been set, the app may be executed on the device by
calling the one of application object's \textit{run methods}.

When an application runs, all values associated with its source object
(all items in a buffer, or or all elements in a range) are passed to
the application as its input stream. The source object is \emph{not
  modified} and may be reused in subsequent runs.  Any results from
the application's execution are \emph{appended} to the end of the
buffers designated as its sinks, which must have sufficient space to
receive them.  Existing data in the sinks' buffers is not overwritten.

MERCATOR will use all available processors of the active device to
execute an application.  \emph{The order of outputs from a sink is not
  guaranteed to match that of the corresponding inputs}, even for
acyclic applications, because multiple GPU blocks asynchronously
process different segments of the input stream and may write their
results to the common output sink out of order.  A future version of
MERCATOR will provide global ordering guarantees for the output
stream.

\subsubsection{Synchronous vs.\ Asynchronous Runs}

An app may be run synchronously using the \texttt{run()} method,
or asynchronously, using the \texttt{runAsync()} method.

A call to \texttt{run()} will not return control to the host CPU
thread until the app (as well as any previous operations on its
stream) has finished.  In contrast, a call to \texttt{runAsync()} will
issue commands to launch an app but may return before it is finished.
The host CPU can then perform work in parallel with the app.  To
ensure that the app has finished its run, the host CPU thread should
call the app's \texttt{join()} method, which blocks until all pending
operations in the the app's stream are finished running.

If one is mixing MERCATOR apps with other CUDA functions, a call to
\texttt{join()} is equivalent to calling
\texttt{cudaStreamSynchronize()} on the app's stream.

\paragraph*{Safety with Asynchronous Operations}

App runs and calls to get, set, copy, or clear buffers are executed
sequentially within a single CUDA stream.  In particular, the
asynchronous versions of these operations will execute sequentially in
the order that they are called, and there is no need to synchronize
the device between calls.  For example, one may safely say
\begin{verbatim}
app.sourceNode.setSource(buffer);
buffer.setAsync(hostPointer1, n1);
app.runAsync();
buffer.setAsync(hostPointer2, n2);
app.runAsync();
join();
\end{verbatim}
The buffer will not be overwritten with the contents of
\texttt{hostPointer2} until after the first run is complete.

It is safe to modify the parameters of an app, including associating
different objects with source and sink nodes using \texttt{setSource}
and \texttt{setSink}, after calling \texttt{runAsync()} but before
calling \texttt{join()}.  Any such changes will take effect with the
next call to \texttt{run()} or \texttt{runAsync()}.  For example,
one may safely say
\begin{verbatim}
app.sourceNode.setSource(buffer1);
app.runAsync();
app.sourceNode.setSource(buffer2);
app.runAsync();
join();
\end{verbatim}
The change of the source buffer to \texttt{buffer2} will not take
effect until the second run.

The principal hazard in asynchronous execution is that the host should
not read or write data that is actively being updated by or
transferred to or from the device.  In particular, once
\texttt{getAsync()} is called on a buffer, the destination data is not
guaranteed to be in its final state until the next \texttt{join()} (or
other synchronous operation).  Similarly, once \texttt{setAsync()} or
\texttt{clearAsync()} is called on a buffer, or a \texttt{runAsync()}
call is issued that uses the buffer as an output sink, the size of the
buffer as seen by the host is not guaranteed to be accurate until the
next \texttt{join()} call.  Moreover, the source data given to a
\texttt{setAsync()} call should not be modified by the host until
after the next \texttt{join()}.

\paragraph*{Limits to Concurrency}

Calls to \texttt{get}, \texttt{set}, \texttt{copy}, or \texttt{clear}
on a buffer object are synchronous -- they do not return until the
operation completes and effectively synchronize the device, since they
execute in the default stream.  To avoid these behaviors, use the
asynchronous versions of the operations.
  
Constructing or destroying a MERCATOR app or a buffer or range object
allocates/frees CUDA device memory, which has the effect of
synchronizing the device.  For best performance, complete all
allocations before any runs of the app, and destroy them after all
runs have finished.

It is not possible to use the same instance of an app to launch
multiple runs on different streams at once, since switching an app to
a new stream first synchronizes the old stream.  If it is desired to
run the app in different streams concurrently, a separate instance
should be created for each stream.


\section{Device-Side App Interface}

The user specifies the behavior of a MERCATOR application by filling
in device function stubs.  Compiling an application specification with
the \texttt{-K} option produces a \textit{skeleton file} containing
these stubs.  This section describes the stubs produced and how they
function as part of an application.

\subsection{Module Run Functions}

The most important functions supplied by the user are the \textit{run
  functions} for each module in an application.  The signature of a
run function is as follows:
\begin{quote}
\texttt{void Module::run(const T \&item, InstTagT nodeIdx)}
\end{quote}
where \texttt{T} is the module's input type.

Each time the run function is called, each active thread is given an
item to process.  The number of items, and hence the number of active
threads, may vary from call to call.  If the module's thread group
size is set to $k > 1$ by a mapping statement, then each consecutive
group of $k$ active threads starting with thread 0 will receive the
same input.

Because multiple nodes may be declared with the same module type, one
call to a module's run function may concurrently process items
destined for different nodes of its type.  Which node received the
input is specified by the \textit{node index argument}
\texttt{nodeIdx}, which is a small integer.  Each item's node index
allows the thread(s) responsible for processing it to select the
parameters, state, and output edges appropriate to its node.

While it is rarely necessary to explicitly test which node an item is
associated with, the mapping from nodes to node indices can be
accessed within a module's functions through an enumerated type in a
namespace \texttt{Node}.  For example, if a module has nodes named
\texttt{A}, \texttt{B}, and \texttt{C}, then the values
\texttt{Node::A}, \texttt{Node::B}, and \texttt{Node::C} specify the
node indices for inputs associated with these nodes.

If a module is declared to run with all threads active (via an
allthreads statement), threads that do not receive an input item will
have their node index argument set to the special value
\texttt{NULLTAG}, and their items will be in an undefined state.  For
such modules, the user should check the node index to ensure that
threads with invalid items do not try to process them.

% Should we rename NULLTAG to something that does not reference the word
% ``tag'', such as NO_IDX?  Then we'd also need to change InstTagT to 
% something else, like NodeIdxT.

\subsubsection{Emitting Output from a Module}

To produce output from a module on one of its channels, call its
\textit{push function} from inside the run function.  the signature
of the push function is as follows:
\begin{quote}
\texttt{void push(const DST \&item, InstTagT nodeIdx, unsigned int channelIdx = 0)}
\end{quote}
where \texttt{DST} is the channel's output type (which may differ from
the input type of the module).  The node index passed to the push function
when emitting an output item should be the same as that of the input
item that produced it.  Channels are indexed using an enumerated type
in a namespace \texttt{Out} within each module.  For example, if a
module has channels named \texttt{acc} and \texttt{rej}, the
corresponding channel indices are \texttt{Out::acc} and
\texttt{Out::rej}.  If no channel index is specified, the first
output channel specified for the module in the specification file
will be used; this default is particularly convenient for modules
with a single output channel.

The push function can be called simultaneously by every active thread
in the run function, or by selected threads based on some predicate.
If the thread group size is greater than 1, only those threads whose
index is 0 modulo the group size will have their items output; other
threads' arguments will be ignored.

\emph{It is essential that a module not push more outputs per thread
  onto a channel than the maximum number given in the channel's
  specification.}  Pushing more outputs than this value will result in
undefined behavior.  Pushing fewer outputs than the maximum is
perfectly fine unless the channel is specified to take a fixed number
of outputs per input with the ``\texttt{!}'' marker. For such
channels, pushing other than the specified number of outputs per input
may result in undefined behavior.

\subsubsection{Accessing Parameters and State}

Within a run function, the user may need to access an application's
parameters or the module's per-node state.  Parameters are
\emph{read-only}, while state is writable.

App-wide parameters are accessible via a function
\texttt{getAppParams}, which returns a pointer to an app-wide
parameter structure identical to that used on the host (except that it
is read-only).  Module-wide \emph{and} per-node parameters are both
accessed by a function \texttt{getParams} which returns a pointer to a
structure containing both kinds of parameters.  Per-module parameters
are again accessed via this structure exactly as on the host.  However,
each per-node parameter is now presented as an \emph{array} of values
indexed by node identifier.

the intended usage idiom of per-node parameters is as follows.
Because each thread may be processing an item from a distinct node,
each thread should use its node index argument to select the correct
member of the per-node parameter's array.  For example, if a module
has a per-node parameter \texttt{x} of type \texttt{int}, a run
function would access this parameter in each thread as
\texttt{getParams()->x[nodeIdx]}.

Mutable per-node state variables of a module are accessed via the
function \texttt{getState}, which returns a pointer to a state
structure.  As for per-node parameters, each per-node state variable
is presented as an array indexed by node identifier.  Because multiple
threads in one call to a run function may be processing items for the
same node, the user must take care when writing to a state variable
that concurrent reads and writes are correctly arbitrated, e.g.\ by
using atomic updates or by ensuring that only one thread per node does
the writing.

% We don't offer decent facilities for users to do collective
% operations segmented by tag right now.  They must either ``roll
% their own'' segmented reductions or use atomics.  This somewhat
% limits the utility of per-node state.  We need to design 
% facilities for this purpose.  Meanwhile, it's probably best to
% have stateful modules have only one instance for simplicity.

\subsubsection{Utility Functions}

The following utility functions are available within a module's
\texttt{run} function.

\begin{itemize}

\item \texttt{unsigned int getNumInstances() const} \\
      Get the number of nodes of this module type.

\item \texttt{unsigned int getNumActiveThreads() const} \\
      Get the \emph{maximum} number of threads with which the run
      function could ever be called.  The actual number of active
      threads in any given call may be less than this value.  The
      value will be equal to the number of threads per block unless
      a lower input limit was specified for the module.

\item \texttt{unsigned int getThreadGroupSize() const} \\
      Get the number of threads in each thread group.

\item \texttt{bool isThreadGroupLeader() const} \\
      True iff a thread is the first thread in its group.

\end{itemize}


\subsection{Other Stub Functions: \texttt{init} and \texttt{cleanup}}

For each module with mutable per-node state, the skeleton defines two
stubs, \texttt{void init()} and \texttt{void cleanup()}.  The first is
called once at the beginning of each run of the application, before
any module's run function is called.  the second is called once at the
end of each run, after the last call to any module's run
function. These functions are intended to permit initialization and
finalization of the module's mutable per-node state.

The \texttt{init} and \texttt{cleanup} functions are always called
with all threads in the block active.  The utility functions
described in the previous section are all available within these
functions as well.

\emph{Note}: calling the module's push function from \texttt{init} or
\texttt{cleanup} is not permitted and may result in undefined
behavior.

\end{document}
